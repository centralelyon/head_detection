{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G_-2jMUK8WY"
      },
      "source": [
        "### Notebook for training a Faster-RCNN model on a custom dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FIwptv1LIm5"
      },
      "source": [
        "This notebook aims to train a Faster-RCNN type model with Pytorch on a custom dataset such as for swimmer head detection. This notebook can be used online (with google Colab for example) or on a local machine. \n",
        "\n",
        "You can find the Pytorch implementation of Faster-RCNN here :  https://pytorch.org/vision/stable/_modules/torchvision/models/detection/faster_rcnn.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DruOhvftLk7V"
      },
      "source": [
        "If you plan to use this notebook with Google Colab you need first to mount your Google Drive by executing the first cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amnWa70AJJG5"
      },
      "source": [
        "# If you use Google Colab\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir('gdrive/My Drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4ftVgXnMU7p"
      },
      "source": [
        "Then we import the different modules.\n",
        "\n",
        "engine, utils and transforms are imported from the .py files you should have in your working directory. They can be found here : https://github.com/pytorch/vision/tree/master/references/detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVV_hvRMLzaZ"
      },
      "source": [
        "# Importing the necessary modules \n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from PIL import Image, ImageDraw\n",
        "import pandas as pd\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T  \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPL-guugMW4-"
      },
      "source": [
        "You also need to fill in the path towards the label and the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz654GFhMItT"
      },
      "source": [
        "path_label = './labels_nageur.csv' #Path to the .csv containing the dataset annotations in Pascal VOC format\n",
        "path_images = './images' #Path to the folder containing the dataset images\n",
        "path_model = './model.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzJICBrUMz1O"
      },
      "source": [
        "We define a function to parse the data. Then we create the class for our dataset, named \"SwimmerDataset\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwQdyg-uMej5"
      },
      "source": [
        "def parse_one_annot(path_to_data_file, filename):\n",
        "    data = pd.read_csv(path_to_data_file)\n",
        "    boxes_array = data[data[\"filename\"] == filename][[\"xmin\", \"ymin\",\"xmax\", \"ymax\"]].values  #annotations are in Pascal VOC format\n",
        "   \n",
        "    return boxes_array\n",
        "\n",
        "class SwimmerDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, data_file, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.imgs = sorted(os.listdir(os.path.join(root, nom_image)))\n",
        "        self.path_to_data_file = data_file\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # load images and bounding boxes\n",
        "        img_path = os.path.join(self.root,nom_image, self.imgs[idx]) \n",
        "        img = Image.open(img_path).convert(\"RGB\") \n",
        "        box_list = parse_one_annot(self.path_to_data_file, \n",
        "        self.imgs[idx])\n",
        "        boxes = torch.as_tensor(box_list, dtype=torch.float32)\n",
        "        num_objs = len(box_list)\n",
        "        \n",
        "        # only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:,\n",
        "        0])\n",
        "        \n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "            return img, target\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pV9cSRMNBdy"
      },
      "source": [
        "We define a function to instantiate the model. We use a **resnet_50fpn pretrained backbone** for the Faster-RCNN.  **Anchor sizes** are an important parameter that you can modify by chaning sizes and spect ratios of *anchor_generator*. Faster-RCNN has many other parameters that have been left at their default values here. These can be found in the link given above for the Faster-RCNN implementation\n",
        "\n",
        "We also define the transform applied to the dataset during training (including data augmentation). These transform are those defined in the transform.py file but it is possible to define new ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs8VGF_vMfzQ"
      },
      "source": [
        "def get_model(num_classes):\n",
        "   # load an object detection model pre-trained on COCO\n",
        "   model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "   # create an anchor_generator for the FPN which by default has 5 outputs\n",
        "   anchor_generator = AnchorGenerator(sizes=((10,), (20,), (30,), (40,), (50,)), aspect_ratios=tuple([(0.5, 1.0, 2.0) for _ in range(5)]))\n",
        "        \n",
        "   # get the number of input features for the classifier\n",
        "   in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    \n",
        "   # replace the pre-trained head with a new on\n",
        "   model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)\n",
        "   \n",
        "   return model\n",
        "\n",
        "def get_transform(train):\n",
        "    transformslist = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transformslist.append(T.ToTensor())\n",
        "\n",
        "    if train:\n",
        "        # data augmentation\n",
        "        transformslist.append(T.RandomHorizontalFlip(0.5))\n",
        "        transformslist.append(T.RandomPhotometricDistort())\n",
        "        transformslist.append(T.RandomZoomOut())\n",
        "\n",
        "    return T.Compose(transformslist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMKji6oONUw_"
      },
      "source": [
        "We split the dataset between training dataset (80%) and validation dataset (20%). We also choose the **batch size** for the training (currently 4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXMX8Z5-MhHS"
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = SwimmerDataset(root= \"swimmer\",\n",
        "          data_file= \"swimmer/\"+nom_csv ,\n",
        "          transforms = get_transform(train=True))\n",
        "\n",
        "dataset_test = SwimmerDataset(root= \"swimmer\",\n",
        "               data_file= \"swimmer/\"+nom_csv ,\n",
        "               transforms = get_transform(train=False))\n",
        "\n",
        "# split the dataset between train and test set\n",
        "\n",
        "#torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "\n",
        "nb_test = int(0.2*len(dataset))\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-nb_test])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-(nb_test):])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader_train = torch.utils.data.DataLoader(\n",
        "              dataset, batch_size=4, shuffle=True, num_workers=0,\n",
        "              collate_fn=utils.collate_fn) #batch_size here\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "         dataset_test, batch_size=1, shuffle=False, num_workers=0,\n",
        "         collate_fn=utils.collate_fn)\n",
        "\n",
        "print(\"The dataset contains {} images : {} for training and {} for validation\".format(len(indices), len(dataset), len(dataset_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7Gy-IlINhu_"
      },
      "source": [
        "We instantiate the model with only 2 classes (the background and the object we want to detect). We use a **learning rate scheduler** to decrease the learning rate along the training. The training is done on the GPU is CUDA is available (leading to a much more faster training) otherwise it is done on the CPU.\n",
        "\n",
        "In this cell we define the **optimizer** (SGD) and its hyperparameters such as the **learning rate**, the **momentum** and the **weight_decay**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeHqaD5LMigu"
      },
      "source": [
        "# use GPU if CUDA is available otherwise use CPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "# get the model \n",
        "model = get_model(num_classes)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.01,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate over epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ludjVYNLNr7W"
      },
      "source": [
        "We finally train our Faster-RCNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AI7BLDJMpBw"
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # train for one epoch\n",
        "  training = train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=40) #print every print_freq batches\n",
        "    \n",
        "  # update the learning rate\n",
        "  lr_scheduler.step()\n",
        "\n",
        "  # evaluate on the test dataset\n",
        "  evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbcu9DSaNuQn"
      },
      "source": [
        "Then we save the weights of the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nIYGBB7Nzyn"
      },
      "source": [
        "#Save the model \n",
        "\n",
        "torch.save(model.state_dict(), path_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}